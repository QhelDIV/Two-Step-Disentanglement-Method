{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.0\n",
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# some setup code\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torchvision.datasets as dset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import layers\n",
    "import params\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "def show_images(images):\n",
    "    images = np.reshape(images, [images.shape[0], -1])  # images reshape to (batch_size, D)\n",
    "    sqrtn = int(np.ceil(np.sqrt(images.shape[0])))\n",
    "    sqrtimg = int(np.ceil(np.sqrt(images.shape[1])))\n",
    "\n",
    "    fig = plt.figure(figsize=(sqrtn, sqrtn))\n",
    "    gs = gridspec.GridSpec(sqrtn, sqrtn)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(img.reshape([sqrtimg,sqrtimg]))\n",
    "    return \n",
    "\n",
    "def preprocess_img(x):\n",
    "    return 2 * x - 1.0\n",
    "\n",
    "def deprocess_img(x):\n",
    "    return (x + 1.0) / 2.0\n",
    "\n",
    "def rel_error(x,y):\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def count_params(model):\n",
    "    \"\"\"Count the number of parameters in the current TensorFlow graph \"\"\"\n",
    "    param_count = np.sum([np.prod(p.size()) for p in model.parameters()])\n",
    "    return param_count\n",
    "\n",
    "USE_GPU = True\n",
    "print(torch.__version__)\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = params.img_size\n",
    "img_channels = params.img_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEnc(nn.Module):\n",
    "    def __init__(self, in_channel, channel_1, channel_2, dense_channel, s_dim):\n",
    "        super().__init__()\n",
    "def Encoder(img_size, in_channel, conv_channel, filter_size, latent_dim, bn):\n",
    "    inner_conv_channel = conv_channel/2\n",
    "    if img_size%4 != 0:\n",
    "        print(\"WARNING: image size mod 4 != 0, may produce bug.\")\n",
    "    flatten_img_size = inner_conv_channel * img_size/4 * img_size/4\n",
    "    model = nn.Sequential(\n",
    "        layers.ConvLayer(in_channel,        conv_channel,       filter_size, stride=2, bn=bn),\n",
    "        layers.ConvLayer(conv_channel,      inner_conv_channel, filter_size, stride=2, bn=bn),\n",
    "        layers.ConvLayer(inner_conv_channel,inner_conv_channel, filter_size, stride=1, bn=bn),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(flatten_img_size, dense_size),\n",
    "        layers.Dense(dense_size,       latent_dim)\n",
    "    )\n",
    "    return model\n",
    "def Classifier(input_dim, dense_size, s_classes, bn):\n",
    "    \n",
    "    model = nn.Sequential(\n",
    "        layers.Dense(input_dim,  dense_size, bn=bn, ),\n",
    "        layers.Dense(dense_size, dense_size, bn=bn),\n",
    "        layers.Dense(dense_size, s_classes,  bn=bn),\n",
    "        nn.Softmax()\n",
    "    )\n",
    "    return model\n",
    "def Decoder(img_size, in_channel, conv_channel, filter_size, bn):\n",
    "    # essentially the mirror version of Encoder\n",
    "    inner_conv_channel = conv_channel/2\n",
    "    flatten_img_size = inner_conv_channel * img_size*img_size\n",
    "    \n",
    "    input_dim = s_dim + latent_dim\n",
    "    \n",
    "    model = nn.Sequential(\n",
    "        layers.Dense(input_dim, dense_size),\n",
    "        layers.ConvLayer(input_dim,         conv_channel,       filter_size, bn=bn),\n",
    "        layers.ConvLayer(conv_channel,      inner_conv_channel, filter_size, bn=bn),\n",
    "        layers.ConvLayer(inner_conv_channel,inner_conv_channel, filter_size, bn=bn),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(flatten_img_size, dense_size),\n",
    "        layers.Dense(dense_size,       latent_dim)\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_Encoder(bn, latent_dim):\n",
    "    # bn: whether use batch normalization in dense layer\n",
    "    conv_channel= params.enc_conv_channel\n",
    "    filter_size = params.enc_conv_filter_size\n",
    "    in_channel  = img_channels\n",
    "    latent_size = params.s_dim\n",
    "    \n",
    "    x = torch.zeros((64, img_channels, img_size, img_size), dtype=dtype)\n",
    "    model = Encoder(img_size, in_channel, conv_channel, latent_dim, filter_size, bn)\n",
    "    scores = model(x)\n",
    "    print(scores.size())  # you should see [64, 10]\n",
    "def test_Decoder():\n",
    "    # TODO\n",
    "    pass\n",
    "\n",
    "#test S encoder\n",
    "test_Encoder(params.s_enc_bn, params.s_enc_dim)\n",
    "#test z encoder\n",
    "test_Encoder(params.z_enc_bn, params.z_enc_dim)\n",
    "#test decoder\n",
    "test_Encoder(params.z_enc_bn, params.z_enc_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
